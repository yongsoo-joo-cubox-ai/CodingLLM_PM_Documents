# 2026-01-15: 프로젝트 소개 및 초기 벤치마크

프로젝트 착수 단계의 기본 소개 자료와 모델 성능 평가 결과를 정리합니다.

> **다음 진행**: [2026-01-24 1월 4주차 진행](../2026-01-24_progress/) - VRAM 산정, MCP 아키텍처, QA 테스트

---

## 주요 내용 요약

- Coco(당시 Coder) 프로젝트 구조 및 기술 스택 소개
- 4개 모델 품질 벤치마크: GPT-OSS 94%, Qwen2.5-32B 54%, Qwen2.5-7B 42%, Mistral-7B 40%
- vLLM 동시사용자 부하 테스트: 1인 25초 → 10인 65초 (배칭 효율 3.8배)

---

## 포함 문서

| 문서 | 설명 |
|------|------|
| `project_introduction.md` | 프로그램 소개 - API 서버 구조, 기술 스택, 테스트 모델 목록 |
| `model_benchmark.md` | 4개 모델 품질 비교 (Task List Screen 생성 기준) |
| `load_test_qwen32b.md` | Qwen2.5-Coder-32B vLLM 부하 테스트 (1~10 동시사용자) |

---

## 핵심 벤치마크 결과

### 모델 품질 (2026-01-14)

| 모델 | 크기 | 품질 점수 | 응답 시간 |
|------|------|----------|----------|
| gpt-oss | 20B | **94%** | ~15s |
| Qwen2.5-32B-AWQ | 32B | 54% | 25s |
| Qwen2.5-7B | 7B | 42% | 11s |
| Mistral-7B | 7B | 40% | 8s |

### 동시사용자 부하 (Qwen2.5-32B, 4x RTX 2080 Ti)

| 동시사용자 | 처리량 | p50 지연 |
|-----------|--------|---------|
| 1 | 0.04 rps | 25.1s |
| 5 | 0.12 rps | 43.5s |
| 10 | 0.15 rps | 62.9s |

---

## 기술 스택 (착수 시점)

- **BE**: Rust 기반 API 서버
- **FE**: React
- **Model**: vLLM + HuggingFace 모델
- **DB**: PostgreSQL
- **추론 서버**: 시선AI 내부 GPU 서버 (4x RTX 2080 Ti)
